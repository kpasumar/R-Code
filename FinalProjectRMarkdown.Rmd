---
title: "STAT 331 Final Project"
author: "Krishna Prem Pasumarthy & Islam Amin"
date: "`r format(Sys.Date(), format = '%B %d, %Y')`"
output:
  pdf_document:
    number_sections: yes
  html_document:
    df_print: paged
---

\section{Summary}

The objective of this report is to come up with a suitable model that best predicts the risk score for CHD based on other variates available in the Framingham Heart Study dataset. [WRITE ABOUT STATISTICAL ANALYSIS]
Stepwise automated model selection is used to create a regression model for the response variate logit(\texttt{chdrisk}) based on other covariates and their interactions. Then after conducting some F-tests, another model is manually constructed by removing some interactions from the stepwise model.Both the models are diagnosed and the latter is selected to have better predictive as well as explanatory power.


\section{Descriptive Statistics}

First, take a look at summary statistics of the Framingham Heart Study dataset.

```{r echo=FALSE}
library(knitr)
suppressWarnings(library(kableExtra))
suppressWarnings(library(gtools))
# Hide NA values from summary statistics which appear for categorical variates
options(knitr.kable.NA = '') 
fhsd <- read.csv('fhs.csv')
# summary statistics on explanatory variables broken into two small tables
kable(summary(fhsd[,1:9]), "latex", caption = "Summary Statistics", booktabs = T) %>%
kable_styling(latex_options = c("striped", "hold_position", "scale_down"))
kable(summary(fhsd[,10:18]), "latex", booktabs = T) %>%     # Table continued on another line
kable_styling(latex_options = c("striped", "hold_position", "scale_down"))
```

First observation we make from the summary is that the median and average ages are around 60, which means the survey seems to have been done on a relatively old group of people. We also have a significantly higher number of females in the study, almost 30% more than the number of males. This might affect the nature of the data to be skewed towards behaviors and physical attributes associated with females.

A further inspection of the mean coronary heart disease (CHD) risk against certain categorical variates, gives more insights.

For instance, if we take a look at mean CHD risk against whether or not an individual has hypertension, we get the following result:

```{r,echo=FALSE}
by(fhsd$chdrisk, fhsd$prevhyp, summary)
```

Indeed, we have that mean CHD risk given that a person has hypertension is significantly higher than the mean for people who did not have hypertension.

```{r,echo=FALSE}
by(fhsd$chdrisk, fhsd$prevstrk, summary)
```


Again, we see the same results with people who had a stroke before the study, with even a higher difference between the two groups.

Now take a look at pair plots of all numeric variates i.e. all variates excluding logical variates such as whether or not currently a cigarette smoker. 

```{r echo=FALSE,fig.width= 8,fig.height=6.5}
# pair plots for continuous variates
pairs(~ chdrisk + totchol + age + 
        sysbp + diabp + cigpday + 
        bmi + heartrte + glucose + 
        hdlc + ldlc, 
      col = "blue",                                         # Change color
      pch = 18,                                            # Change shape of points
      cex = 0.4,
      gap = 1,
      main = "Pair Plots of Continuous Variates",
      data = fhsd)
```

From the pair plots, we can observe a strong linear relationship between low density lipoprotein cholesterol and serum total cholestrol. As total cholestrol increases, low density lipoprotein cholosterol seems to increase as well. Another positive correlation can be observed between systolic and diastolic blood pressures. In fact, from this we can infer that blood pressure probably increases and decreases generally for both systolic and diastolic states at the same time.

Now take a look at the VIFs of these variates.


```{r echo=FALSE}
# design matrix excluding intercept
X <- model.matrix(lm(chdrisk ~ . -  1, data = fhsd))
# remove linearly dependent column (sexMale = 1 - sexFemale)
X <- X[,-1]
# calculate vif
vif <- diag(solve(cor(X)))
kable(t(round(vif,3)),"latex",booktabs = T,caption = "VIFs of Variates") %>%
kable_styling(latex_options = c("striped", "hold_position","scale_down"),font_size = 11)
```

We observe VIF values higher than 10 for both serum total cholestrol and low density lipoprotein cholesterol, which means they have significant colinearity with other variates.

\section{Candidate Models}

\subsection{Automated Model Selection}

In this section we start producing a candidate model using automated model selection. Here, we choose to use a stepwise as we have observed from lectures that it usually acts as a compromise between backward and forward selection methods. This way, we avoid having a lot of variates in our final model relatively and also we capture as many necessary variates as possible.

We first try our initial and maximum models as follows.

```{r echo=FALSE}
load_calcs = TRUE
# model with only intercept
M0 <- lm(I(logit(chdrisk)) ~ 1, data = fhsd)
M0$call
# model with all interactions
Mmax <- lm(I(logit(chdrisk)) ~ (.)^2, data = fhsd)
Mmax$call
```

However, we end up getting NAs in the coefficients for two interactions namely: whether currently a cigrette smoker and number of cigarettes smoked each day, whether individual is on anti-hypertensive medication and whether the individual actually has hypertension.

```{r, eval=FALSE, include=FALSE}

# find model coefficients which are NA
beta.max <- coef(Mmax)
names(beta.max)[is.na(beta.max)]

```

If we investigate the relationship between these variables as shown below, we see that those who do not smoke have no cigarettes per day making these two respective columns linearly dependent.

Furthermore, if someone does not have hypertension, they would not use anti-hypertensive medication causing a linear dependence between these two columns.

```{r echo=FALSE}
# find the problem with the NA coeffs
kable(table(fhsd[c("cursmoke", "cigpday")]), "latex", caption = "cursmoke against cigpday", booktabs = T) %>% 
kable_styling(latex_options = c("striped", "hold_position", "scale_down"))
kable(table(fhsd[c("bpmeds", "prevhyp")]), "latex", caption = "bpmeds against prevhyp", booktabs = T)  %>%    
kable_styling(latex_options = c("striped", "hold_position"),font_size = 6)
```

To fix these we remove these two interactions from the maximal model and add some quadratic terms for continuous variates in hope of having some additional predicitive power.

```{r,echo= FALSE}
# remove the coeffs with the problem and add quadratic terms for the continuous variables
Mmax <- lm(I(logit(chdrisk)) ~ (.)^2 - cursmoke:cigpday - bpmeds:prevhyp + 
             I(totchol ^ 2) + I(sysbp ^ 2) + I(diabp ^ 2) 
           + I(bmi ^ 2) + I(glucose ^ 2)
           + I(hdlc ^ 2) + I(ldlc ^ 2), data = fhsd)
Mmax$call

load_calcs = TRUE
# starting model for stepwise selection
Mstart <- lm(I(logit(chdrisk)) ~ ., data = fhsd) 
if(!load_calcs){
  #stepwise model selection
  system.time({
    Mstep <- step(object = Mstart,
                  scope = list(lower = M0, upper = Mmax),
                  direction = "both", trace = FALSE)
  })
}
# the caching/loading block
if(!load_calcs) {
  saveRDS(list(Mfwd = Mfwd, Mback = Mback, Mstep = Mstep), file = "models_automated.rds") 
} else {
  # just load the calculations
  tmp <- readRDS("models_automated.rds")
  Mstep <- tmp$Mstep
  rm(tmp) # optionally remove tmp from workspace
}
```

Finally, we produce the following model using stepwise model selection:

```{r echo=FALSE}
beta.step <- coef(Mstep) # Store coefficients of the stepwise model
Mstep$call
```


\subsection{Manual Model Selection}

The following table lists terms in the stepwise model that result in insignifance when an F-test is perfomed by removing them solely from the model along with corresponding p-values in a sorted order.

```{r,echo=FALSE}
library(stringr)                                  # For string operations
table <- c()                                      # Initialize empty vector
names.table <- names(beta.step)                   # Obtain variate names in stepwise model 
names.table <- str_remove_all(names.table,"Yes")  # Remove "Yes" from interactions
names.table <- str_remove_all(names.table,"Male") # Remove "Male" from interactions

# Perform F-tests with Mstep by removing one variate at a time 
for(i in names.table){
  # Obtain model without variate i
  mdl <- lm(as.formula(paste0("update(Mstep, . ~ . -", i,")")),data = fhsd)
  test <- anova(Mstep,mdl)               # F-Test between Stepwise and reduced model
  table <- cbind(table,test$`Pr(>F)`[2]) # Add corresponding p-value to the table
}

table <- as.data.frame(table)
colnames(table) <- names.table           # Add appropriate column names to the table
table <- sort(table,decreasing = TRUE)   # Arrange variates by decreasing insignificance

sig_values <- which(table >= 0.05)      # Insignificant p-values from the sorted table
kable(table[,sig_values[1:9]],"latex", caption = "Variates/Interactions with insignificant p-values from F-test", 
      booktabs = T) %>%
kable_styling(latex_options = c("striped", "hold_position","scale_down"))
kable(table[,sig_values[10:17]],"latex",booktabs = T) %>%
kable_styling(latex_options = c("striped", "hold_position","scale_down"))
```

Looking at the above table, removing highly insignificant continuous variate interactions between heart rate and number of cigarettes per day, between diastolic blood pressure and number of cigarettes per day we have the following p-value from F-test.

```{r}
# Remove as many insignificant continuous variate interactions as possible
anova(Mstep, update(Mstep,. ~ . - cigpday:heartrte - diabp:cigpday))$`Pr(>F)`[2]
```

Assuming the insignificance threshold of 0.05, removing categorical/continuous variate interaction between whether an individual is on anti-hypertensive medication and whether an individual has had the stroke results in the following p-value.

```{r}
# Now remove insignificant interactions from categroical variates
anova(Mstep, update(Mstep,. ~ . - cigpday:heartrte - diabp:cigpday- bpmeds:prevstrk))$`Pr(>F)`[2]
```

Since above p-value is just slightly greater than 0.05, removing the above 3 interactions from stepwise model is insiginificant. Therefore a reduced model can be obtained from stepwise in the following way.

```{r,echo=FALSE}
# Thus we have the following manually constructed model
Mmanual <- update(Mstep,. ~ . - cigpday:heartrte - diabp:cigpday - bpmeds:prevstrk)   
Mmanual$call
```

\section{Model Diagnostics}

\subsection{Residual Plots}

In this section we analyse the assumption that our residuals follow a normal distribution and check the homoscedasticity assumption.

First, we have that the most normal looking residuals assuming that the model is true, would be the studentized residuals on the standard deviation scale, so to check the homoscedasticity assumption we plot those values against the predicted values, as shown below:

```{r echo=FALSE}
# First we analyze Mstep
# get the hat values
sigma.hat <- sigma(Mstep)
h <- hatvalues(Mstep)
res.step <- resid(Mstep)/(sigma.hat * sqrt(1-h)) # studentized residuals, on the standard deviation scale
cex <- .8 # controls the size of the points and labels
plot(predict(Mstep), res.step, pch = 21, bg = "blue", cex = cex, cex.axis = cex, 
     main = "StepWise Model",
     xlab = "Predicted logit(CHD Risk)", ylab = "Studentized Residuals")
abline(h = 0, lty = 1, col = "black") # add horizontal line at 0

sigma.hat <- sigma(Mmanual)
# Then we analyze Mdl_manual
# get the hat values
h <- hatvalues(Mmanual)
res.manual <- resid(Mmanual)/(sigma.hat * sqrt(1-h)) # studentized residuals, but on the data scale
cex <- .8 # controls the size of the points and labels
plot(predict(Mmanual), res.manual, pch = 21, bg = "red", cex = cex, cex.axis = cex,
     main = "Manually Constructed Model",
     xlab = "Predicted logit(CHD Risk)", ylab = "Studentized Residuals")
abline(h = 0, lty = 1, col = "black") # add horizontal line at 0
```

Analysis of the plots reveals that both models have very similar residual distributions, and for both, there seems to be a pattern of decreasing spread of residuals as the predicted logit value increases. Hence, we can conclude that both models are based on a violated homoscedasity assumption, i.e., in light of the observed data there seems to be a change in the standard deviation of the response variate as the explanatory variables change.


Then to check our assumption of normality of residuals we plot the residuals on a histogram and a QQPlot:

```{r echo=FALSE,fig.width= 8,fig.height=4}
# plot standardized residuals
cex <- .8
par(mfrow = c(1,2),oma = c(0,0,2,0))
# histogram
hist(res.manual, breaks = 50, freq = FALSE, cex.axis = cex,
     xlab = "Studentized Residual logit(CHD Risk)",main = "")

curve(dnorm(x), col = "red", add = TRUE) # theoretical normal curve

#qq-plot
qqnorm(res.manual,pch = 16, cex = cex, cex.axis = cex,main = "")
abline(a = 0, b = 1, col = "red") # add 45 degree line
mtext(expression(bold("Manually Constructed Model")),outer = TRUE,cex = 1.5,at = c(0.5,-1))

# plot standardized residuals
sigma.hat <- sigma(Mstep)
par(mfrow = c(1,2),oma = c(0,0,2,0))
# histogram
hist(res.step, breaks = 50, freq = FALSE, cex.axis = cex,
     xlab = "Studentized Residual logit(CHD Risk)", main = "")

curve(dnorm(x), col = "red", add = TRUE) # theoretical normal curve

#qq-plot
qqnorm(res.step, main = "", pch = 16, cex = cex, cex.axis = cex)
abline(a = 0, b = 1, col = "red") # add 45 degree line
mtext(expression(bold("StepWise Model")),outer = TRUE,cex = 1.5,at = c(0.5,-1))
```


Again, from both plots we see a huge similarity between both models, and for both we seem to have an approximately normal distribution being satisfied by the residuals. From the QQPlot, we can observe that most observations lie on the theoretical line.

From this diagnostics there seems to not be a significant departure from our assumptions of homoscedasticity and normality of residuals.

\subsection{Leverage and Influence Measures}

We have the following boxplot of absolute values of leverages of both the step-wise and manual models.

```{r,echo=FALSE,fig.width=6,fig.height=4}
Mnames <- expression(M[Step], M[Manual])

hat1 <- hatvalues(Mstep) # Leverages of stepwise model
hat2 <- hatvalues(Mmanual)

# Create a box plot of absolute leverages of both models
boxplot(x = list(abs(hat1), abs(hat2)), names = Mnames, main = "Absolute Leverages",
        ylab = "Abs. Leverages", col = c("yellow", "orange"))
```

Similarly, we have the following boxplot of cook's distances for both the models.

```{r,echo=FALSE,fig.width=6,fig.height=4}
# Similarly compute and plot cook's distance for both the models
cook1 <- cooks.distance(Mstep)
cook2 <- cooks.distance(Mmanual)

boxplot(x = list(abs(cook1), abs(cook2)), names = Mnames, main = "Cook's Distance",
        ylab = "Cooks Distance", col = c("yellow", "orange"))

```

From the first plot above, we see that leverage for most observations is far from the desired value of 1 in both the models. Whereas from the second plot, cook's distance for most of the observations is close to desired value 0. And finally, both the models have very similar values for leverages and cook's distances.

\section{Model Selection}

\subsection{Cross Validation}

```{r,echo=FALSE}
library(statmod) # Load this package for using gauss.quad.prob() function

#' Following function calculates the mean of logit-normal distribution
#' 
#' @param mu Mean of underlying normal distribution
#' @param sigma Standard deviation of underlying normal distribution
#' 
#' @return A single number representing mean of the logit-normal distribution
#' 
#' @details The calculation of w's and g(x)'s is vectorized
logitnorm_mean <- function(mu,sigma){
  v = 1/(1+ exp(-mu))           # Value passed into both shape parameters
  alpha_1 = 1/(sigma^2 * (1-v)) # Shape parameter 1
  alpha_2 = 1/(v * sigma^2) # Shape parameter 2
  # Calculate nodes and weights for Gaussian quadrature
  gqp <- gauss.quad.prob(n = 10,dist = "beta",alpha = alpha_1,beta = alpha_2)
  x <- gqp$nodes   # Extract the nodes into a vector
  w <- gqp$weights # Similarly the weights
  # Apply the function g (defined in the project description) onto the above x's
  g <- dnorm(logit(x),mean = mu,sd = sigma,log = TRUE) - log(1-x) - 
       dbeta(x,shape1 = alpha_1,shape2 = alpha_2,log = TRUE)
  # Calculate and return the mean
  answer <- sum(w*exp(g)) 
  return(answer)
}
```

Before performing cross-valiation analysis, function \texttt{logitnorm$\_$mean} is created to approximate the conditional mean \texttt{E[chdrisk|x]} based on the regression model logit(\texttt{chdrisk})|x $\sim  N(x'\beta,\sigma^2)$ (look into the Appendix for code). The following output is produced when tested.

```{r}
# Test the function
mu <- c(0.7,3.2,-1.1)
sigma <- c(0.8,0.1,2.3)
# Returns results expected in the project desciption
sapply(1:3, function(i) logitnorm_mean(mu[i],sigma[i]))

```

The above function is then used to perform cross-validation analysis and the following boxplot that shows MSPE of the both the models is produced.

```{r,echo=FALSE}
load_calcs = TRUE

M1 <- Mstep
M2 <- Mmanual

# number of cross-validation replications
nreps <- 1e3

ntot <- nrow(fhsd)   # total number of observations
ntrain <- 1800       # for fitting MLE's, roughly 80% of total
ntest <- ntot-ntrain # for out-of-sample prediction

# storage space
mspe1 <- rep(NA, nreps) # mspe for M1
mspe2 <- rep(NA, nreps) # mspe for M2

if (!load_calcs){
system.time({
  for(ii in 1:nreps) {
    train.ind <- sample(ntot, ntrain) # training observations
    
    # Update the models for this training set
    M1.cv <- update(M1, subset = train.ind)
    M2.cv <- update(M2, subset = train.ind)
    
    # MLE of sigma
    M1.sigma <- sqrt(sum(resid(M1.cv)^2)/ntrain) 
    M2.sigma <- sqrt(sum(resid(M2.cv)^2)/ntrain)
    
    # predictions of logit(chdrisk) for test set
    predictions.M1 <- predict(M1.cv,newdata = fhsd[-train.ind,])
    predictions.M2 <- predict(M2.cv,newdata = fhsd[-train.ind,])
    
   # predictions of chdrisk for the test set
   values.M1 <- sapply(predictions.M1, function(i) logitnorm_mean(i,M1.sigma))
   values.M2 <- sapply(predictions.M2, function(i) logitnorm_mean(i,M2.sigma))
    
    M1.res <- fhsd$chdrisk[-train.ind] -   # test observations
              values.M1                    # prediction using training data
    M2.res <- fhsd$chdrisk[-train.ind] - values.M2
    
    # mspe for each model
    mspe1[ii] <- mean(M1.res^2)
    mspe2[ii] <- mean(M2.res^2)
    
  }
})
}

# the caching/loading block
if(!load_calcs) {
  saveRDS(list(mspe1 = mspe1,mspe2 = mspe2), file = "cross_validation_automated.rds") 
} else {
  # just load the calculations
  tmp <- readRDS("cross_validation_automated.rds")
  mspe1 <- tmp$mspe1
  mspe2 <- tmp$mspe2
  rm(tmp) # optionally remove tmp from workspace
}

# compare Root MSPEs of both the models through boxplots
boxplot(x = list(sqrt(mspe1), sqrt(mspe2)), names = Mnames,
        main = "Root MSPE",
        ylab = expression(sqrt(MSPE)),
        col = c("yellow", "orange"))
```

From previous discussions about residuals, leverages and influence measures, both models seem to be almost identical. It can observed from the above boxplot that the manually constructed model has slightly lower MSPE values than the stepwise model and therefore has slightly better predictive power. Also, the manual model has lesser covariate interactions than the stepwise model and hence has better explanatory power as well. Therefore, we select the manually constructed model over stepwise.

These are the parameter estimates, std.errors and p-values of the manually constructed model.

```{r,echo=FALSE}
coeffs <- summary(Mmanual) # Obtain summary of manually constructed model

# Function to create a table of parameters estimates, std.errors and p-values for 10 columns
call_kable <- function(i){
if(i != 1) caption = NULL
else caption = "Summary of chosen manually constructed model"
kable(t(round(coeffs$coefficients[i:(i+8),c(1,2,4)],4)), "latex", caption = caption, booktabs = T) %>%
kable_styling(latex_options = c("striped", "hold_position","scale_down"))
}

call_kable(1)
call_kable(10)
call_kable(19)
call_kable(28)
call_kable(37)
call_kable(46)
call_kable(55)

```

\section{Discussion}

From our analysis, we make the following conclusions :

1. Any conclusions we make might be more biased towards females due to the signifcantly higher females than males in the study.
    
2. From our pair plots we can observe that high HDL cholestrol levels have people with less risk for heart disease. So it seems like higher         HDL cholestrol levels are associated with less expected CHD risk. This effect can be supported by the relatively low p-value for the             significance of hdlc in the presence of other variates in the model. On the contrary, people who have had previous mycordial infractions,        strokes, or hypertension seem to have a higher expected CHD risk, as shown by the relatively high positive estimated coefficients and very       low p-values. 
    
3. We can observe from the model diagnostics of the chosen model that there might be a slight departure from the assumption of                      homoscedasticity. In fact, it seems as though the standard deviation decreases as we get into the region of people with high risk of             coronary heart disease. This might be due to the fact that we have relatively more variates that have a positive association with higher         CHD risk and also less people with high CHD risk, hence we have more predictive power when presented with a person with factors of high          CHD risk. However, for people with low CHD, it might be harder to predict their CHD risk due to more variation that arises from having a         lot more of people with relatively low CHD risk in the study. Therefore, our conclusions regarding factors that relate to high CHD risk          might be stronger than those that relate to lower CHD risks.

4. From the box plot of cook's distance for the manual model, it can be seen that one specific individual has an unusually high value. The following command finds that outlying observation and the corresponding cook's distance.

```{r}
cook2[which.max(cook2)]
```

Since 0.78 is an unusually high cook's distance value, the 916th individual can be excluded from the analysis.

5. This is an observational study, which means certain conclusions regarding causations can not be made. Therefore, suggesting any behavioural changes based on this report would not be justified because non of the conclusions we established are causal, e.g. we can not say that smoking less would result in a decreased risk of heart disease.

6. Finally, we do observe some variates retained in the final model with high p-values, meaning that they might be insignificant. However, removing those variates does fail an F-test with 5% significance, therefore we can not say with confidence that they are insiginificant. This might be due to other variates being significant in the presence of those variates with high p-values.