---
title: "STAT 331 Final Project"
author: "Krishna Prem Pasumarthy & Islam Amin"
date: "`r format(Sys.Date(), format = '%B %d, %Y')`"
output:
  pdf_document:
    number_sections: yes
  html_document:
    df_print: paged
---

\section{Summary}

The objective of this report is to come up with a suitable model that best predicts the risk score for CHD based on other variates available in the Framingham Heart Study dataset. [WRITE ABOUT STATISTICAL ANALYSIS]
Stepwise automated model selection is used to create a regression model for the response variate logit(\texttt{chdrisk}) based on other covariates and their interactions. Then after conducting some F-tests, another model is manually constructed by removing some interactions from the stepwise model.Both the models are diagnosed and the latter is selected to have better predictive as well as explanatory power.


\section{Descriptive Statistics}

First, take a look at summary statistics of the Framingham Heart Study dataset.

```{r echo=FALSE}
library(knitr)
suppressWarnings(library(kableExtra))
suppressWarnings(library(gtools))
# Hide NA values from summary statistics which appear for categorical variates
options(knitr.kable.NA = '') 
fhsd <- read.csv('fhs.csv')
# summary statistics on explanatory variables broken into two small tables
kable(summary(fhsd[,1:9]), "latex", caption = "Summary Statistics", booktabs = T) %>%
kable_styling(latex_options = c("striped", "hold_position", "scale_down"))
kable(summary(fhsd[,10:18]), "latex", booktabs = T) %>%     # Table continued on another line
kable_styling(latex_options = c("striped", "hold_position", "scale_down"))
```

First observation we make from the summary is that the median and average ages are around 60, which means the survey seems to have been done on a relatively old group of people. We also have a significantly higher number of females in the study, almost 30% more than the number of males. This might affect the nature of the data to be skewed towards behaviors and physical attributes associated with females.

A further inspection of the expected coronary heart disease (CHD) risk against certain categorical variates, gives more insights.

For instance, if we take a look at expected CHD risk against whether or not an individual has hypertension, we get the following result:

```{r,echo=FALSE}
by(fhsd$chdrisk, fhsd$prevhyp, summary)
```

Indeed, we have that mean CHD risk given that a person has hypertension is significantly higher than the mean for people who did not have hypertension.

```{r,echo=FALSE}
by(fhsd$chdrisk, fhsd$prevstrk, summary)
```


Again, we see the same results with people who had a stroke before the study, with even a higher difference between the two groups.

Now take a look at pair plots of all numeric explanatory/response variates i.e. variates excluding logical variates such as \texttt{cursmoke}. 

```{r echo=FALSE,fig.width= 6,fig.height=6.5}
# pair plots for continuous variates
pairs(~ chdrisk + totchol + age + 
        sysbp + diabp + cigpday + 
        bmi + heartrte + glucose + 
        hdlc + ldlc, 
      col = "blue",                                         # Change color
      pch = 18,                                            # Change shape of points
      cex = 0.4,
      gap = 1,
      main = "Pair Plots of Continuous Variates",
      data = fhsd)
```

From the pair plots, we can observe a strong correlation between low density lipoprotein cholesterol and serum total cholestrol. As total cholestrol increases, low density lipoprotein cholosterol seems to increase as well. Another positive correlation can be observed between systolic and diastolic blood pressures. In fact, from this we can infer that blood pressure probably increases and decreases generally for both systolic and diastolic states.

Now take a look at the VIFs of these variates.


```{r echo=FALSE}
# design matrix excluding intercept
X <- model.matrix(lm(chdrisk ~ . -  1, data = fhsd))
# remove linearly dependent column (sexMale = 1 - sexFemale)
X <- X[,-1]
# calculate vif
vif <- diag(solve(cor(X)))
vif
```

We observe higher VIF values higher than 10 for both serum total cholestrol and low density lipoprotein cholesterol, which means they are significantly colinear.

\section{Candidate Models}

\subsection{Automated Model Selection}

In this section we start producing a candidate model using automated model selection. Here, we choose to use a stepwise as we have observed from lectures that it usually acts as a compromise between backward and forward selection methods. This way, we avoid having relatively a lot of variates in our final model and also we capture as many necessary variates as possible.

We first try our initial and maximum models as follows,

```{r echo=FALSE}
load_calcs = TRUE
# model with only intercept
M0 <- lm(I(logit(chdrisk)) ~ 1, data = fhsd)
# model with all interactions
Mmax <- lm(I(logit(chdrisk)) ~ (.)^2, data = fhsd)
```

However, we end up getting NAs in the coefficients for two interactions (Currently smoking with cigarettes per day and anti-hypertensive medication with previous hypertension).

```{r eval=FALSE, include=FALSE}

# find model coefficients which are NA
beta.max <- coef(Mmax)
names(beta.max)[is.na(beta.max)]

```

If we investigate the relationship between these variables as shown below, we see that those who do not smoke have no cigarattes per day making the two columns cursmoke and cigpday linearly dependent.

Furthermore, if someone did not hypertension, they would not use anti-hypertensive medication causing a linear dependence between these two columns.

```{r echo=FALSE}
# find the problem with the NA coeffs
kable(table(fhsd[c("cursmoke", "cigpday")]), "latex", caption = "cursmoke against cigpday", booktabs = T) %>% 
kable_styling(latex_options = c("striped", "hold_position", "scale_down"))
kable(table(fhsd[c("bpmeds", "prevhyp")]), "latex", caption = "cursmoke against cigpday", booktabs = T)  %>%    
kable_styling(latex_options = c("striped", "hold_position"))
```

To fix these we remove these two interactions from Mmax to retain the following model:

```{r}

# remove the coeffs with the problem and add quadratic terms for the continuous variables
Mmax <- lm(I(logit(chdrisk)) ~ (.)^2 - cursmoke:cigpday - bpmeds:prevhyp + 
             I(totchol ^ 2) + I(sysbp ^ 2) + I(diabp ^ 2) 
           + I(bmi ^ 2) + I(glucose ^ 2)
           + I(hdlc ^ 2) + I(ldlc ^ 2), data = fhsd)

```

We also add some quadratic terms for continuous variates in hope of having some additional predicitive power.

```{r echo=FALSE}

load_calcs = TRUE
# starting model for stepwise selection
Mstart <- lm(I(logit(chdrisk)) ~ ., data = fhsd) 
if(!load_calcs){
  #forward model selection
  system.time({
    Mfwd <- step(object = M0,
                  scope = list(lower = M0, upper = Mmax),
                  direction = "forward", trace = FALSE)
  })
  
  #backward model selection
  system.time({
    Mback <- step(object = Mmax,
                  scope = list(lower = M0, upper = Mmax),
                  direction = "backward", trace = FALSE)
  })
  
  #stepwise model selection
  system.time({
    Mstep <- step(object = Mstart,
                  scope = list(lower = M0, upper = Mmax),
                  direction = "both", trace = FALSE)
  })
}
# the caching/loading block
if(!load_calcs) {
  saveRDS(list(Mfwd = Mfwd, Mback = Mback, Mstep = Mstep), file = "models_automated.rds") 
} else {
  # just load the calculations
  tmp <- readRDS("models_automated.rds")
  Mfwd <- tmp$Mfwd
  Mback <- tmp$Mback
  Mstep <- tmp$Mstep
  rm(tmp) # optionally remove tmp from workspace
}
# Forward model selection
#Mfwd$call
# Backward model selection
#Mback$call
#beta.fwd = coef(Mfwd)
#beta.back = coef(Mback)
```

Finally, we produce the following model using stepwise model selection:

```{r echo=FALSE}
beta.step = coef(Mstep)
Mstep$call
#identical(names(beta.fwd)[names(beta.fwd) %in% names(beta.back)], names(beta.fwd))
#identical(names(beta.fwd)[names(beta.fwd) %in% names(beta.step)], names(beta.fwd))
#identical(names(beta.back)[names(beta.back) %in% names(beta.step)], names(beta.back))
```



\subsection{Manual Model Selection}

The following table lists terms in the stepwise model that result in insignifance when an F-test is perfomed by removing them solely from the model along with corresponding p-values in a sorted order.

```{r,echo=FALSE}
library(stringr)                                  # For string operations
table <- c()                                      # Initialize empty vector
names.table <- names(beta.step)                   # Obtain variate names in stepwise model 
names.table <- str_remove_all(names.table,"Yes")  # Remove "Yes" from interactions
names.table <- str_remove_all(names.table,"Male") # Remove "Male" from interactions

# Perform F-tests with Mstep by removing one variate at a time 
for(i in names.table){
  # Obtain model without variate i
  mdl <- lm(as.formula(paste0("update(Mstep, . ~ . -", i,")")),data = fhsd)
  test <- anova(Mstep,mdl)               # F-Test between Stepwise and reduced model
  table <- cbind(table,test$`Pr(>F)`[2]) # Add corresponding p-value to the table
}

table <- as.data.frame(table)
colnames(table) <- names.table           # Add appropriate column names to the table
table <- sort(table,decreasing = TRUE)   # Arrange variates by decreasing insignificance

sig_values <- which(table >= 0.05)      # Insignificant p-values from the sorted table
kable(table[,sig_values[1:9]],"latex", caption = "Variates/Interactions with insignificant p-values from F-test", 
      booktabs = T) %>%
kable_styling(latex_options = c("striped", "hold_position","scale_down"))
kable(table[,sig_values[10:17]],"latex",booktabs = T) %>%
kable_styling(latex_options = c("striped", "hold_position","scale_down"))
```

Looking at the above table, removing highly insignificant continuous variate interactions \texttt{cigpday:heartrte} and \texttt{diabp:cigpday}, we have the following p-value from F-test.

```{r}
# Remove as many insignificant continuous variate interactions as possible
anova(Mstep, update(Mstep,. ~ . - cigpday:heartrte - diabp:cigpday))$`Pr(>F)`[2]
```

Assuming the insignificance threshold of 0.05, removing categorical/continuous variate interaction \texttt{bpmeds:prevstrk} results in the following p-value.

```{r}
# Now remove insignificant interactions from categroical variates
anova(Mstep, update(Mstep,. ~ . - cigpday:heartrte - diabp:cigpday- bpmeds:prevstrk))$`Pr(>F)`[2]
```

Since above p-value is just slightly greater than 0.05, removing the above interactions from stepwise model is insiginificant. Therefore a reduced model can be obtained from stepwise in the followng way.

```{r}
# Thus we have the following manually constructed model
Mmanual <- update(Mstep,. ~ . - cigpday:heartrte - diabp:cigpday - bpmeds:prevstrk)   
```

\section{Model Diagnostics}

\subsection{Residual Plots}

In this section we analyse the assumption that our residuals follow a normal distribution and check the homoscedasticity assumption.

First, we have that the most normal looking residuals assuming that the model is true, would be the studentized residuals on the standard deviation scale, so to check the homoscedasticity assumption we plot those values against the predicted values, as shown below:

```{r echo=FALSE}
# First we analyze Mstep
# get the hat values
sigma.hat <- sigma(Mstep)
h <- hatvalues(Mstep)
res.step <- resid(Mstep)/(sigma.hat * sqrt(1-h)) # studentized residuals, on the standard deviation scale
cex <- .8 # controls the size of the points and labels
par(mar = c(4,4,.5,.1))
plot(predict(Mstep), res.step, pch = 21, bg = "blue", cex = cex, cex.axis = cex,xlab = "Predicted logit(CHD Risk - Stepwise Model)", ylab = "Studentized Residuals")
abline(h = 0, lty = 1, col = "black") # add horizontal line at 0

sigma.hat <- sigma(Mmanual)
# Then we analyze Mdl_manual
# get the hat values
h <- hatvalues(Mmanual)
res.manual <- resid(Mmanual)/(sigma.hat * sqrt(1-h)) # studentized residuals, but on the data scale
cex <- .8 # controls the size of the points and labels
par(mar = c(4,4,.5,.1))
plot(predict(Mmanual), res.manual, pch = 21, bg = "red", cex = cex, cex.axis = cex,xlab = "Predicted logit(CHD Risk - Manual Model)", ylab = "Studentized Residuals")
abline(h = 0, lty = 1, col = "black") # add horizontal line at 0
```

Analysis of the graphs reveals that both models have very similar residual distributions, and for both, there seems to be a pattern of decreasing spread of residuals as the predicted logit value increases. Hence, we can conclude that both models are based on a violated homoscedasity assumption, i.e., in light of the observed data there seems to be a change in the standard deviation of the response variate as the explanatory variables change.


Then to check our assumption of normality of residuals we plot the residuals on a QQPlot and a histogram:

```{r echo=FALSE,fig.width= 8,fig.height=4}

# plot standardized residuals

cex <- .8
par(mfrow = c(1,2), mar = c(4,4,.1,.1))
# histogram
hist(res.manual, breaks = 50, freq = FALSE, cex.axis = cex,xlab = "Studentized Residual CHD Risk (Manual)", main = "")

curve(dnorm(x), col = "red", add = TRUE) 
# theoretical normal curve
#qq-plot
qqnorm(res.manual, main = "", pch = 16, cex = cex, cex.axis = cex)
abline(a = 0, b = 1, col = "red") # add 45 degree line

# plot standardized residuals
sigma.hat <- sigma(Mstep)
cex <- .8
par(mfrow = c(1,2), mar = c(4,4,.1,.1))
# histogram
hist(res.step, breaks = 50, freq = FALSE, cex.axis = cex,xlab = "Studentized Residual CHD Risk (Stepwise)", main = "")

curve(dnorm(x), col = "red", add = TRUE) 
# theoretical normal curve
#qq-plot
qqnorm(res.step, main = "", pch = 16, cex = cex, cex.axis = cex)
abline(a = 0, b = 1, col = "red") # add 45 degree line

```


Again, from both plots we see a huge similaruty between both models, and for both we seem to have a normal distribution being satisfied by the residuals. From the QQPlot, we can observe that most points lie on the theoretical line.

From this diagnostics there seems to not be a significant departure from our assumptions of homoscedasticity and normality of residuals.

\subsection{Leverage and Influence Measures}

We have the following boxplot of absolute values of leverages of both the step-wise and manual models.

```{r,echo=FALSE}
Mnames <- expression(M[Step], M[Manual])

hat1 <- hatvalues(Mstep) # Leverages of stepwise model
hat2 <- hatvalues(Mmanual)

# Create a box plot of absolute leverages of both models
boxplot(x = list(abs(hat1), abs(hat2)), names = Mnames,
        ylab = "Abs. Leverages", col = c("yellow", "orange"))
```

Similarly, we have the following boxplot of cook's distances for both the models.

```{r,echo=FALSE}
# Similarly compute and plot cook's distance for both the models
cook1 <- cooks.distance(Mstep)
cook2 <- cooks.distance(Mmanual)

boxplot(x = list(abs(cook1), abs(cook2)), names = Mnames,
        ylab = "Cooks Distances", col = c("yellow", "orange"))

```

From the first plot above, we see that leverage for most observations is far from the desired value of 1 in both the models. Whereas from the second plot, cook's distance for most of the observations is close to desired value 0. And finally, both the models have very similar values for leverages and cook's distances.

\section{Model Selection}

\subsection{Cross Validation}

```{r,echo=FALSE}
library(statmod) # Load this package for using gauss.quad.prob() function

#' Following function calculates the mean of logit-normal distribution
#' 
#' @param mu Mean of underlying normal distribution
#' @param sigma Standard deviation of underlying normal distribution
#' 
#' @return A single number representing mean of the logit-normal distribution
#' 
#' @details The calculation of w's and g(x)'s is vectorized
logitnorm_mean <- function(mu,sigma){
  v = 1/(1+ exp(-mu))           # Value passed into both shape parameters
  alpha_1 = 1/(sigma^2 * (1-v)) # Shape parameter 1
  alpha_2 = 1/(v * sigma^2) # Shape parameter 2
  # Calculate nodes and weights for Gaussian quadrature
  gqp <- gauss.quad.prob(n = 10,dist = "beta",alpha = alpha_1,beta = alpha_2)
  x <- gqp$nodes   # Extract the nodes into a vector
  w <- gqp$weights # Similarly the weights
  # Apply the function g (defined in the project description) onto the above x's
  g <- dnorm(logit(x),mean = mu,sd = sigma,log = TRUE) - log(1-x) - 
       dbeta(x,shape1 = alpha_1,shape2 = alpha_2,log = TRUE)
  # Calculate and return the mean
  answer <- sum(w*exp(g)) 
  return(answer)
}
```

Before performing cross-valiation analysis, function \texttt{logitnorm$\_$mean} is created to approximate the conditional mean \texttt{E[chdrisk|x]} based on the regression model logit(\texttt{chdrisk})|x $\sim  N(x'\beta,\sigma^2)$ (look into the Appendix for code). The following output is produced when tested.

```{r}
# Test the function
mu <- c(0.7,3.2,-1.1)
sigma <- c(0.8,0.1,2.3)
# Returns results expected in the project desciption
sapply(1:3, function(i) logitnorm_mean(mu[i],sigma[i]))

```

The above function is then used to perform cross-validation analysis and the following boxplot that shows MSPE of the both the models is produced.

```{r,echo=FALSE}
load_calcs = TRUE

M1 <- Mstep
M2 <- Mmanual

# number of cross-validation replications
nreps <- 1e3

ntot <- nrow(fhsd)   # total number of observations
ntrain <- 1800       # for fitting MLE's, roughly 80% of total
ntest <- ntot-ntrain # for out-of-sample prediction

# storage space
mspe1 <- rep(NA, nreps) # mspe for M1
mspe2 <- rep(NA, nreps) # mspe for M2

if (!load_calcs){
system.time({
  for(ii in 1:nreps) {
    train.ind <- sample(ntot, ntrain) # training observations
    
    # Update the models for this training set
    M1.cv <- update(M1, subset = train.ind)
    M2.cv <- update(M2, subset = train.ind)
    
    # MLE of sigma
    M1.sigma <- sqrt(sum(resid(M1.cv)^2)/ntrain) 
    M2.sigma <- sqrt(sum(resid(M2.cv)^2)/ntrain)
    
    # predictions of logit(chdrisk) for test set
    predictions.M1 <- predict(M1.cv,newdata = fhsd[-train.ind,])
    predictions.M2 <- predict(M2.cv,newdata = fhsd[-train.ind,])
    
   # predictions of chdrisk for the test set
   values.M1 <- sapply(predictions.M1, function(i) logitnorm_mean(i,M1.sigma))
   values.M2 <- sapply(predictions.M2, function(i) logitnorm_mean(i,M2.sigma))
    
    M1.res <- fhsd$chdrisk[-train.ind] -   # test observations
              values.M1                    # prediction using training data
    M2.res <- fhsd$chdrisk[-train.ind] - values.M2
    
    # mspe for each model
    mspe1[ii] <- mean(M1.res^2)
    mspe2[ii] <- mean(M2.res^2)
    
  }
})
}

# the caching/loading block
if(!load_calcs) {
  saveRDS(list(mspe1 = mspe1,mspe2 = mspe2), file = "cross_validation_automated.rds") 
} else {
  # just load the calculations
  tmp <- readRDS("cross_validation_automated.rds")
  mspe1 <- tmp$mspe1
  mspe2 <- tmp$mspe2
  rm(tmp) # optionally remove tmp from workspace
}

# compare Root MSPEs of both the models through boxplots
boxplot(x = list(sqrt(mspe1), sqrt(mspe2)), names = Mnames,
        main = "Root MSPE",
        ylab = expression(sqrt(MSPE)),
        col = c("yellow", "orange"))

# compare predictions by training set; is this needed?
# par(mar = c(5, 5, 2, 1))
# plot(mspe1, mspe2, pch = 16,
#      xlab = Mnames[1], ylab = Mnames[2],
#      main = paste0("MSPE of ", Mnames[1]," vs ", Mnames[2])) # Fix this
# abline(a = 0, b = 1, col= "red", lwd = 2)
```

Model chosen will be the manual one [ADD MORE]
These are the parameter estimates, std.errors and p-values of the manually constructed model.

```{r,echo=FALSE}
coeffs <- summary(Mmanual) # Obtain summary of manually constructed model

# Function to create a table of parameters estimates, std.errors and p-values for 10 columns
call_kable <- function(i){
if(i != 1) caption = NULL
else caption = "Summary of chosen manually constructed model"
kable(t(coeffs$coefficients[i:(i+8),c(1,2,4)]), "latex", caption = caption, booktabs = T) %>%
kable_styling(latex_options = c("striped", "hold_position", "scale_down"))
}

call_kable(1)
call_kable(10)
call_kable(19)
call_kable(28)
call_kable(37)
call_kable(46)
call_kable(55)

```

\section{Discussion}



